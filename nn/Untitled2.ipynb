{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network with one hidden layer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def loadCSV (filePath, skip=1) :\n",
    "    return np.asarray(pd.read_csv(filePath, skiprows=skip, header=None))\n",
    "\n",
    "\n",
    "train = loadCSV('../train.csv')\n",
    "test = loadCSV('../test.csv')\n",
    "\n",
    "featureSize = 785\n",
    "hiddenSize = 100\n",
    "outputSize = 10\n",
    "\n",
    "dataSize = 42000\n",
    "testSize = 28000\n",
    "\n",
    "reg = 5.0\n",
    "\n",
    "maxIterations = 200\n",
    "iteration = 1\n",
    "cachedCost = 0\n",
    "\n",
    "def sigmoid (x) :\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoidGrad (x) :\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def predict (x, data, m, featureSize, hiddenSize, outputSize) :\n",
    "    hiddenTheta = x[0 : featureSize * (hiddenSize - 1)].reshape((featureSize, (hiddenSize - 1)))\n",
    "    outputTheta = x[featureSize * (hiddenSize - 1) : featureSize * (hiddenSize - 1) + hiddenSize * outputSize].reshape((hiddenSize, outputSize))\n",
    "    \n",
    "    # m x featureSize\n",
    "    features = np.ones((m, featureSize))\n",
    "    features[:, 1:] = data / 255.0\n",
    "\n",
    "    # m x hiddenLayerSize\n",
    "    hiddenLayer = np.ones((m, hiddenSize))\n",
    "    hiddenLayer[:, 1:] = sigmoid(features.dot(hiddenTheta))\n",
    "    \n",
    "    # m x \n",
    "    outputLayer = sigmoid(hiddenLayer.dot(outputTheta))\n",
    "    \n",
    "    return np.argmax(outputLayer, 1)\n",
    "\n",
    "def getCost (x, *args):\n",
    "    data, featureSize, hiddenSize, outputSize, reg = args\n",
    "    m = np.size(data, 0)\n",
    "    \n",
    "    hiddenTheta = x[0 : featureSize * (hiddenSize - 1)].reshape((featureSize, (hiddenSize - 1)))\n",
    "    outputTheta = x[featureSize * (hiddenSize - 1) : featureSize * (hiddenSize - 1) + hiddenSize * outputSize].reshape((hiddenSize, outputSize))\n",
    "    \n",
    "    # m x 1\n",
    "    labels = data[:, 0]\n",
    "    expectedValues = np.eye(outputSize)[labels]\n",
    "    \n",
    "    # m x featureSize\n",
    "    features = np.ones((m, featureSize))\n",
    "    features[:, 1:] = data[:, 1:] / 255.0\n",
    "\n",
    "    # m x hiddenSize\n",
    "    hiddenLayer = np.ones((m, hiddenSize))\n",
    "    hiddenLayer[:, 1:] = sigmoid(features.dot(hiddenTheta))\n",
    "    \n",
    "    # m x outputSize\n",
    "    outputLayer = sigmoid(hiddenLayer.dot(outputTheta))\n",
    "    \n",
    "    J = -np.sum(expectedValues * np.log(outputLayer + 1e-8) + (1 - expectedValues) * np.log(1 - outputLayer + 1e-8)) / m\n",
    "    J += reg / (2 * m) * (np.sum(hiddenTheta[:, 1:] ** 2) + np.sum(outputTheta[:, 1:] ** 2))\n",
    "    \n",
    "    # m x outputSize\n",
    "    err3 = (outputLayer - expectedValues) * 1 # sigmoidGrad(hiddenLayer.dot(outputLayerTheta))\n",
    "    \n",
    "    # m x (hiddenSize - 1)\n",
    "    err2 = err3.dot(outputTheta.transpose())[:, 1:] * sigmoidGrad(features.dot(hiddenTheta))\n",
    "    \n",
    "    # hiddenSize x outputSize\n",
    "    grad2 = hiddenLayer.transpose().dot(err3) / m + reg * np.concatenate((np.zeros((hiddenSize, 1)), outputTheta[:, 1:]), axis = 1) / m\n",
    "    \n",
    "    # featureSize x (hiddenSize - 1)\n",
    "    grad1 = features.transpose().dot(err2) / m + reg * np.concatenate((np.zeros((featureSize, 1)), hiddenTheta[:, 1:]), axis = 1) / m\n",
    "    \n",
    "    global cachedCost\n",
    "    cachedCost = J\n",
    "    return (J, np.concatenate((grad1.flatten(), grad2.flatten()), axis=0))\n",
    "\n",
    "def getNumericalGradient (getCost, theta, args) :\n",
    "    epsilon = 1e-5\n",
    "    numgrad = np.zeros(np.size(theta))\n",
    "    for i in range(np.size(theta, 0)):\n",
    "        oldT = theta[i]\n",
    "        theta[i] = oldT + epsilon\n",
    "        pos = getCost(theta, *args)[0]\n",
    "        theta[i] = oldT - epsilon\n",
    "        neg = getCost(theta, *args)[0]\n",
    "        numgrad[i] = (pos - neg) / (2 * epsilon)\n",
    "        theta[i] = oldT\n",
    "    return numgrad\n",
    "\n",
    "def callback (xk) :\n",
    "    global iteration\n",
    "    print iteration\n",
    "    print cachedCost\n",
    "    iteration += 1\n",
    "\n",
    "args = (train, featureSize, hiddenSize, outputSize, reg)\n",
    "\n",
    "x0 = np.random.rand(featureSize * (hiddenSize - 1) + hiddenSize * outputSize) / 100.0\n",
    "# print np.sum((getCost(x0, *args)[1] - getNumericalGradient(getCost, x0, args)) ** 2)\n",
    "x1 = minimize(fun=getCost, x0=x0, method=\"CG\", options={\"maxiter\": maxIterations, \"disp\":True}, jac=True, args=args, callback=callback).x\n",
    "\n",
    "np.savetxt(\"../results.csv\", predict(x1, test, testSize, featureSize, hiddenSize, outputSize), delimiter=\",\", fmt=\"\\\"%d\\\"\")\n",
    "np.savetxt(\"../theta.csv\", x1, delimiter=\",\", fmt=\"%f\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
